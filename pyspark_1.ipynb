{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "874dc45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "88a42ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0227978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "873c46d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "03cbb803",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"c:\\\\Users\\\\ninve\\\\Downloads\\\\CROSS_BORDER_COMMERCIAL_SCHEDULE_201501010000-201601010000_BG.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b965ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(file_path, header = [3,4,5], na_values=['N/E', 'Nan']).dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "85deb384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [col[1].replace(' ', \"_\") if col[0].startswith(\"Unnamed\") else col[0].replace(' > ', '_') + '_' + col[1].replace('-', '_') + '_' + col[2].replace(r' \\[|\\] ', '')  for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5a1c97a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6827e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn(\"raw_date\", when(col(\"Time_Interval\").contains('.'), col(\"Time_Interval\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "175823b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.orderBy(monotonically_increasing_id()).rowsBetween(Window.unboundedPreceding, 0)\n",
    "spark_df = spark_df.withColumn(\"r_date\", last(\"raw_date\", ignorenulls=True).over(w))\n",
    "spark_df = spark_df.withColumn(\"date\", when(col(\"r_date\").isNull(), '01.01.2015').otherwise(col(\"r_date\")))\n",
    "spark_df = spark_df.withColumn(\"date\", to_date(col(\"date\"), 'dd.MM.yyyy')) \\\n",
    "    .filter(col(\"Time_Interval\").contains('-')).drop(\"r_date\", \"raw_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "12bb4bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = [\"date\", \"Time_Interval\"]\n",
    "value_cols = [c  for c in spark_df.columns if c not in id_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "af6d93b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = spark_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3998df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = pd_df.melt(\n",
    "    id_vars=id_cols,\n",
    "    value_vars=value_cols,\n",
    "    var_name='Direction',\n",
    "    value_name='MW'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "64bbe31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df = spark.createDataFrame(df_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ed5b542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------+\n",
      "|      Date|           Direction|MW_per_month|\n",
      "+----------+--------------------+------------+\n",
      "|2015-01-01| RS_BG_Intraday_[MW]|       103.0|\n",
      "|2015-01-01|MK_BG_Day_ahead_[MW]|         0.0|\n",
      "|2015-01-01|GR_BG_Day_ahead_[MW]|         0.0|\n",
      "|2015-01-01|BG_GR_Day_ahead_[MW]|     14400.0|\n",
      "|2015-01-01|BG_MK_Day_ahead_[MW]|      4579.0|\n",
      "|2015-01-01|RS_BG_Day_ahead_[MW]|       103.0|\n",
      "|2015-01-01|BG_RS_Day_ahead_[MW]|      3353.0|\n",
      "|2015-01-01| BG_RS_Intraday_[MW]|      3353.0|\n",
      "|2015-01-02|GR_BG_Day_ahead_[MW]|         0.0|\n",
      "|2015-01-02|BG_GR_Day_ahead_[MW]|     14400.0|\n",
      "|2015-01-02|MK_BG_Day_ahead_[MW]|         0.0|\n",
      "|2015-01-02| RS_BG_Intraday_[MW]|       290.0|\n",
      "|2015-01-02|BG_MK_Day_ahead_[MW]|      4819.0|\n",
      "|2015-01-02|RS_BG_Day_ahead_[MW]|       290.0|\n",
      "|2015-01-02|BG_RS_Day_ahead_[MW]|      3644.0|\n",
      "|2015-01-02| BG_RS_Intraday_[MW]|      3644.0|\n",
      "|2015-01-03|GR_BG_Day_ahead_[MW]|         0.0|\n",
      "|2015-01-03|BG_MK_Day_ahead_[MW]|      5013.0|\n",
      "|2015-01-03|BG_GR_Day_ahead_[MW]|     14400.0|\n",
      "|2015-01-03|MK_BG_Day_ahead_[MW]|         0.0|\n",
      "+----------+--------------------+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "sp_df.groupBy(\"Date\", \"Direction\").agg(sum('MW').alias(\"MW_per_month\")).orderBy(\"date\").dropna().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
